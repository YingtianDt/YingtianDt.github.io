
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PDF477WKKG"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-PDF477WKKG');
    </script>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc-markdown-css-theme" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Yingtian Tang</title>
  <link rel="stylesheet" href="template.css" />
</head>
<body>

<header>
<h1 class="title">Yingtian Tang</h1>
<blockquote class="metadata">
</blockquote>
</header>


<main>
<p><img src="img/photo.jpg" style="float:right; height:350px; margin-left:80px" />I am currently a <strong>PhD student</strong> at <a href="https://www.epfl.ch/en/">EPFL</a>, working on NeuroAI with <a href="https://mschrimpf.com/">Prof. Martin Schrimpf</a>.</p>
<p>I had my <strong>Master's degree</strong> at <a href="https://www.upenn.edu/">University of Pennsylvania</a>, majoring in Computer and Information Science (CIS). I was also a research assistant at the <a href="https://www.grasp.upenn.edu">GRASP Lab</a>, supervised by <a href="https://pratikac.github.io/">Prof. Pratik Chaudhari</a>.
<p>I had my <strong>undergraduate</strong> at <a href="https://en.uestc.edu.cn/"> University of Electronic Science and Technology of China</a>, majored in Computer Science.
During my undergraduate, I also worked as research assistants in <a href="https://www.ntu.edu.sg/"> Nanyang Technological University</a> (<a href="https://www.ntu.edu.sg/rose">Rapid-Rich Object Search Lab</a>), and later <a href="https://www.tencent.com/zh-cn/">Tencent</a> (<a href="https://www.tencent.com/en-us/business/robotics.html">Robotics X</a>). </p>

<h2 id="contact">Contact</h2>
<p><a href="mailto:yingtian.tang@epfl.ch">yingtian.tang [at] epfl dot ch</a></p>
<p><a href="pub/CV.pdf">Curriculum Vitae</a> (<a href="pub/UPENN_transcript.pdf">Transcript</a>) [slightly outdated]</p>
<h2 id="research-interests">Research Interests</h2>
<p>
    I am interested in artifitial intelligence, especially how current machine learning methods could approach <strong>active perception and learning</strong> in the real world. I think this line of research lies in the intersection between <strong>computational neuroscience</strong>, <strong>computational cognitive science</strong>, and <strong>machine learning</strong>. 
    I then studied and was impressed by the research by people like <a href="https://en.wikipedia.org/wiki/James_J._Gibson">James J. Gibson</a> and <a href="https://en.wikipedia.org/wiki/Peter_Dayan">Peter Dayan</a>, and believe that machine learning will take them further (or conversely, they will guide a right way for ML research).
</p>
<!-- <p>Before this, I have gone through multiple fields to study machine learning broadly, including graph learning, visual representation learning, reinforcement learning for optimization, and motion synthesis (in time order; please see <a href="pub/CV.pdf">CV</a> for more details).</p> -->
<p>
    Currently, I am working on building models for <strong>dynamic perceptual representation</strong>, based on state-of-the-art representation learning methods, such as JEPA and MAE, and brain recordings of video watching.
</p>

<h2 id="highlights">Publications</h2>
<!-- <div style="display: flex; margin: 25px">
    <img src="img/photo.jpg" style="height:80px; margin-right:25px" />
    <div>
        <strong>Title</strong> <a>Link</a>
        <br>
        Author1, Author2, ..., Authorn
        <br>
        Venue
    </div>
</div> -->

<div style="display: flex; margin: 25px">
    <a href="https://www.biorxiv.org/content/10.1101/2025.07.22.664908v1" style="margin-right:25px"><img src="img/tang2025many.png" style="height:200px;" /></a>
    <div>
        <strong>Diverse Perceptual Representations Across Visual Pathways Emerge from A Single Objective</strong> 
        <br>
        <em>Yingtian Tang</em>, Abdulkadir Gokce, Khaled Jedoui Al-Karkari, Daniel Yamins, Martin Schrimpf
        <br>
        <i>bioRxiv</i>
        <br>
        <a href="https://www.biorxiv.org/content/10.1101/2025.07.22.664908v1">Link</a>,
        <a href="https://x.com/yingtian80536/status/1950541165721215219">Tweets</a>,
        <a href="https://yingtiandt.github.io/dynamic-vision-demo/">Page</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <a href="https://arxiv.org/abs/2503.01830" style="margin-right:25px"><img src="img/alkhamissi2025language.png" style="height:200px;" /></a>
    <div>
        <strong>From Language to Cognition: How LLMs Outgrow the Human Language Network</strong> 
        <br>
        Badr AlKhamissi, Greta Tuckute, <em>Yingtian Tang</em>, Taha Binhuraib, Antoine Bosselut*,Martin Schrimpf*
        <br>
        <i>CCN 2025</i>
        <br>
        <a href="https://arxiv.org/abs/2503.01830">Link</a>,
        <a href="https://x.com/bkhmsi/status/1897312258621161568">Tweets</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <a href="https://arxiv.org/abs/2411.00828" style="margin-right:25px"><img src="img/babyllama_2024.jpg" style="height:200px;" /></a>
    <div>
        <strong>Dreaming Out Loud: A Self-Synthesis Approach For Training Vision-Language Models With Developmentally Plausible Data</strong> 
        <br>
        <em>Yingtian Tang*</em>, Badr AlKhamissi*, Abdulkadir Gokce*, Johannes Mehrer, Martin Schrimpf
        <br>
        <i>BabyLM Challenge, at CoNLL 2024</i>
        <br>
        <a href="https://arxiv.org/abs/2411.00828">Link</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <a href="https://arxiv.org/pdf/2203.16393v1.pdf" style="margin-right:25px"><img src="img/mtcn.png" style="height:200px;" /></a>
    <div>
        <strong>Online Motion Style Transfer for Interactive Character Control</strong> 
        <br>
        <em>Yingtian Tang</em>, Jiangtao Liu, Cheng Zhou, and Tingguang Li
        <br>
        <i>Arxiv Preprint (2021)</i>
        <br>
        <a href="https://arxiv.org/pdf/2203.16393v1.pdf">Link</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <a href="https://arxiv.org/abs/2106.07288" style="margin-right:25px"><img src="img/storage.png" style="height:200px;" /></a>
    <div>
        <strong>Learning-Aided Heuristics Design for Storage System</strong> 
        <br>
        <em>Yingtian Tang</em>, Han Lu, Xijun Li, Lei Chen, Mingxuan Yuan, and Jia Zeng
        <br>
        <i>The 2021 ACM SIGMOD/PODS International Conference on Management of Data</i>
        <br>
        <a href="https://arxiv.org/abs/2106.07288">Link</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <a href="https://ieeexplore.ieee.org/document/9774516" style="margin-right:25px"><img src="img/smaug.png" style="height:200px;" /></a>
    <div>
        <strong>Accurate probabilistic miss ratio curve approximation for adaptive cache allocation in block storage systems</strong> 
        <br>
        Rongshang Li, <em>Yingtian Tang</em>, Qiquan Shi, Hui Mao, Lei Chen, Jikun Jin, Peng Lu, and Zhuo Cheng
        <br>
        <i>2022 Design, Automation & Test in Europe Conference & Exhibition (DATE)</i>
        <br>
        <a href="https://ieeexplore.ieee.org/document/9774516">Link</a>
    </div>
</div>

<div style="display: flex; margin: 25px">
    <iframe height="200" style="margin-right:25px" src="https://www.youtube.com/embed/im7L-KuSzIA" title="Visual Analytic System for Pandemic Management During COVID-19" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    <div>
        <strong>Visual Analytic System for Pandemic Management During COVID-19</strong> 
        <br>
        Shan Lin, Fu Long Tan, Chen Hongyu, Kuan Yang Tang, <em>Yingtian Tang</em>, Nemath Ahmed, and Alex Kot
        <br>
        <i>ICIP 2020 IEEE Signal Processing Society 5-Minute Video Clip Contest Winner</i>
        <br>
        <a href="https://www.youtube.com/watch?v=im7L-KuSzIA">Video</a>
    </div>
</div>

</main>

<script>
;(function() {
  // Non-essential if user has JavaScript off. Just makes checkboxes look nicer.
  var selector = '.task-list > li > input[type="checkbox"]';
  var checkboxes = document.querySelectorAll(selector);
  Array.from(checkboxes).forEach((checkbox) => {
    var wasChecked = checkbox.checked;
    checkbox.disabled = false;
    checkbox.addEventListener('click', (ev) => {ev.target.checked = wasChecked});
  });
})();
</script>
</body>
</html>
